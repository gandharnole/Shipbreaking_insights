{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Decza6xZmf2j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_2015 = pd.read_csv(\"ships_2015.csv\")\n",
        "df_2016 = pd.read_csv(\"ships_2016.csv\")\n",
        "\n",
        "# Impute missing LDT using 0.8 × GT\n",
        "for df in [df_2015, df_2016]:\n",
        "    df['LDT'] = df.apply(\n",
        "        lambda row: row['LDT'] if pd.notnull(row['LDT']) else round(0.8 * row['GT'], 0),\n",
        "        axis=1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_2015 = pd.read_csv(\"ships_2015.csv\")\n",
        "print(df_2015.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfeJC_cHpCVp",
        "outputId": "02e3ffda-ae3d-4925-9c32-071ce6ee35ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['YEAR', 'IMO', 'NAME', 'TYPE', 'GT', 'LDT', 'BUILT', 'LAST_FLAG', 'PREVIOUS_FLAG', 'BENEFICIAL_OWNER', 'BO_COUNTRY', 'COMMERCIAL_OPERATOR', 'REGISTERED_OWNER', 'RO_COUNTRY', 'PLACE', 'COUNTRY', 'ARRIVAL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_2015.columns = df_2015.columns.str.strip().str.upper()\n",
        "df_2016.columns = df_2016.columns.str.strip().str.upper()\n"
      ],
      "metadata": {
        "id": "yTBDiC1VpRIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def format_arrival(date_str):\n",
        "    try:\n",
        "        parsed = pd.to_datetime(date_str, format='%d-%m-%Y', errors='coerce')\n",
        "        if pd.notnull(parsed):\n",
        "            return parsed.strftime('%d-%b')  # e.g., 18-May\n",
        "        else:\n",
        "            return date_str  # keep original if can't parse\n",
        "    except:\n",
        "        return date_str\n",
        "\n",
        "for df in [df_2015, df_2016]:\n",
        "    df['ARRIVAL'] = df['ARRIVAL'].astype(str).apply(format_arrival)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OWPZsJ-DnCmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the 2015 dataset\n",
        "df_2015 = pd.read_csv(\"/content/ships_2015.csv\")\n",
        "\n",
        "# Convert column names to uppercase for consistency\n",
        "df_2015.columns = df_2015.columns.str.strip().str.upper()\n",
        "\n",
        "# Check if LDT column exists\n",
        "if \"LDT\" in df_2015.columns:\n",
        "    # Impute missing LDT values using median of the column\n",
        "    df_2015[\"LDT\"] = df_2015[\"LDT\"].fillna(df_2015[\"LDT\"].median())\n",
        "else:\n",
        "    print(\"⚠️ No 'LDT' column found in dataset!\")\n",
        "\n",
        "# Save cleaned 2015 dataset\n",
        "df_2015.to_csv(\"/content/ships_2015_cleaned.csv\", index=False)\n",
        "print(\"✅ LDT values imputed and file saved: /content/ships_2015_cleaned.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egY-E3OcuLS8",
        "outputId": "e8f7aff1-ade6-4ff8-e99f-5253cac55796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LDT values imputed and file saved: /content/ships_2015_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_2015.to_csv(\"ships_2015_cleaned.csv\", index=False)\n",
        "df_2016.to_csv(\"ships_2016_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "W28Qcs4Opb9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# --- Step 1: Load your CSV files ---\n",
        "df_2015 = pd.read_csv(\"ships_2015_cleaned.csv\")\n",
        "df_2016 = pd.read_csv(\"ships_2016_cleaned.csv\")\n",
        "df_2022_2024 = pd.read_csv(\"ships_2022_2024.csv\")\n",
        "\n",
        "# --- Step 2: Clean column names ---\n",
        "for df in [df_2015, df_2016, df_2022_2024]:\n",
        "    df.columns = df.columns.str.strip().str.upper()\n",
        "\n",
        "# --- Step 3: Align columns ---\n",
        "common_cols = list(set(df_2015.columns) & set(df_2016.columns) & set(df_2022_2024.columns))\n",
        "df_2015 = df_2015[common_cols]\n",
        "df_2016 = df_2016[common_cols]\n",
        "df_2022_2024 = df_2022_2024[common_cols]\n",
        "\n",
        "# --- Step 4: Merge and sort ---\n",
        "merged_df = pd.concat([df_2015, df_2016, df_2022_2024], ignore_index=True)\n",
        "merged_df = merged_df.sort_values(by=\"YEAR\").reset_index(drop=True)\n",
        "\n",
        "# --- Step 5: Save merged CSV ---\n",
        "output_path = \"ships_2015_2024_complete.csv\"\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "print(\"✅ Merged file created:\", output_path)\n",
        "\n",
        "# --- Step 6: Download to your computer ---\n",
        "files.download(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "AWcF14mWrhgY",
        "outputId": "40c64830-9cbe-4de7-8906-5109f3f2fceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Merged file created: ships_2015_2024_complete.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b0df7b3e-49c6-4355-966e-96f11156079c\", \"ships_2015_2024_complete.csv\", 597479)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Load the merged dataset (replace with your actual merged file name)\n",
        "merged = pd.read_csv(\"/content/ships_2015_2024_complete.csv\")\n",
        "\n",
        "# Desired column order\n",
        "cols = [\n",
        "    \"YEAR\",\"IMO\",\"NAME\",\"TYPE\",\"GT\",\"LDT\",\"BUILT\",\n",
        "    \"LAST_FLAG\",\"PREVIOUS_FLAG\",\"BENEFICIAL_OWNER\",\"BO_COUNTRY\",\n",
        "    \"COMMERCIAL_OPERATOR\",\"REGISTERED_OWNER\",\"RO_COUNTRY\",\n",
        "    \"PLACE\",\"COUNTRY\",\"ARRIVAL\"\n",
        "]\n",
        "\n",
        "# Reorder columns\n",
        "merged = merged[cols]\n",
        "\n"
      ],
      "metadata": {
        "id": "jqLQmL7LtNJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/ships_complete.csv\")\n",
        "\n",
        "# Standardize column names\n",
        "df.columns = df.columns.str.strip().str.upper()\n",
        "\n",
        "# Convert GT and LDT to numeric safely\n",
        "df[\"GT\"] = pd.to_numeric(df[\"GT\"], errors=\"coerce\")\n",
        "df[\"LDT\"] = pd.to_numeric(df[\"LDT\"], errors=\"coerce\")\n",
        "\n",
        "# Impute missing LDT = 0.8 * GT\n",
        "missing_before = df[\"LDT\"].isna().sum()\n",
        "df.loc[df[\"LDT\"].isna(), \"LDT\"] = 0.8 * df.loc[df[\"LDT\"].isna(), \"GT\"]\n",
        "missing_after = df[\"LDT\"].isna().sum()\n",
        "\n",
        "print(f\"✅ Imputed {missing_before - missing_after} missing LDT values using 0.8 × GT.\")\n",
        "\n",
        "# Save and download\n",
        "output_path = \"/content/ships_complete_imputed.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "files.download(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YeLpzNsS6St_",
        "outputId": "d0766ff3-c6d7-4527-853e-f81db93b4a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Imputed 823 missing LDT values using 0.8 × GT.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f507d9f-e7ac-47e6-aa08-3a6588b19fac\", \"ships_complete_imputed.csv\", 592753)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"ships_complete.csv\")\n",
        "\n",
        "# Clean GT column: remove commas, spaces, and convert to float\n",
        "df['GT'] = (\n",
        "    df['GT']\n",
        "    .astype(str)\n",
        "    .str.replace(',', '', regex=False)\n",
        "    .str.strip()\n",
        "    .replace('', '0')  # handle blanks\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "# Convert LDT to numeric (in case it’s stored as text)\n",
        "df['LDT'] = pd.to_numeric(df['LDT'], errors='coerce')\n",
        "\n",
        "# Check missing before\n",
        "print(\"Missing LDTs before:\", df['LDT'].isna().sum())\n",
        "\n",
        "# Impute only missing LDTs with 0.8 * GT\n",
        "mask = df['LDT'].isna()\n",
        "df.loc[mask, 'LDT'] = 0.8 * df.loc[mask, 'GT']\n",
        "\n",
        "# Check after\n",
        "print(\"Missing LDTs after:\", df['LDT'].isna().sum())\n",
        "\n",
        "# Save cleaned dataset\n",
        "df.to_csv(\"ships_all_complete_imputed.csv\", index=False)\n",
        "print(\"✅ Imputation complete and saved as 'ships_all_complete_imputed.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhAPvtP5oHdX",
        "outputId": "15c8423c-f7d8-4001-f292-1611aec4442f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing LDTs before: 1891\n",
            "Missing LDTs after: 8\n",
            "✅ Imputation complete and saved as 'ships_complete_imputed.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 3 Start:"
      ],
      "metadata": {
        "id": "8ek-oQNfBsNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# 1️⃣ Load dataset\n",
        "df = pd.read_csv(\"/content/ships_complete_imputed.csv\")\n",
        "df.columns = df.columns.str.strip().str.upper()\n",
        "\n",
        "# 2️⃣ Create REGION target variable\n",
        "south_asia = ['India', 'Bangladesh', 'Pakistan']\n",
        "df['REGION'] = df['COUNTRY'].apply(lambda x: 'South Asia' if x in south_asia else 'Other')\n",
        "\n",
        "# 3️⃣ Select features\n",
        "features = [\"LDT\", \"TYPE\", \"BUILT\", \"LAST_FLAG\", \"GT\"]\n",
        "X = df[features].copy()  # use copy to avoid SettingWithCopyWarning\n",
        "y = df[\"REGION\"]\n",
        "\n",
        "# 4️⃣ Clean numeric columns (remove commas and convert to float)\n",
        "for col in [\"LDT\", \"GT\", \"BUILT\"]:\n",
        "    X[col] = X[col].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "\n",
        "# 5️⃣ Encode categorical variables\n",
        "label_cols = [\"TYPE\", \"LAST_FLAG\"]\n",
        "encoder = LabelEncoder()\n",
        "for col in label_cols:\n",
        "    X[col] = encoder.fit_transform(X[col].astype(str))\n",
        "\n",
        "# 6️⃣ Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 7️⃣ Train Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 8️⃣ Evaluate\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(f\"Accuracy: {acc:.3f}\")\n",
        "print(f\"F1-Score: {f1:.3f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# 9️⃣ Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=['South Asia', 'Other'])\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['South Asia', 'Other'], yticklabels=['South Asia', 'Other'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# 🔟 Plot class distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='REGION', data=df)\n",
        "plt.title(\"Class Distribution in Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "WsQ7PXd-Bvjd",
        "outputId": "c82185be-a66a-47da-fe32-b3fb63de17e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nGaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-611296797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# 7️⃣ Train Naive Bayes model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[1;32m    265\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nGaussianNB does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    }
  ]
}